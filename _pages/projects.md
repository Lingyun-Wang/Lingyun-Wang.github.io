---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
redirect_from:

---

{% include base_path %}

**CVSR-Net: Axial super-resolution OCT via complex-valued network**  
Axial resolution is important for OCT as a higher axial resolution can reveal more detialed structures, thus helping research and diagnosis. Many researchers have proposed advanced networks to super-resolve OCT but all these methods are real-valued and they ignore the phase of OCT signal. Herein, inspired by complex-valued networks, we proposed the first deep-learning-based super-resolution approach using both amplitude and phase of OCT signal to reconstruct OCT images. The introduction of phase achieves better performance than a network of the same structure without phase introduction.  
**(A) Complex-valued convolution layer** To perform the convolution in complex domain, for a complex-valued input image ![](http://latex.codecogs.com/svg.latex?I=x+iy) and a complex-valued convolution kernel ![](http://latex.codecogs.com/svg.latex?W=A+iB), the complex-valued convolution (ℂConv) can be expressed as ![](http://latex.codecogs.com/svg.latex?W*I=(A*x-B*y)+i(B*x+A*y)). This complex-valued convolution layer can be realized by utilizing two real-valued 2D convolution layers in Pytorch.  
**(B) Complex-valued activation** Complex-valued activation can be obtained by splitting the activation function, and then conducting the activation by adopting the real-valued activation. Thus, the complex-valued ReLU (ℂReLU) is given by:  ![](http://latex.codecogs.com/svg.latex?mathbb{C}\text{ReLU}(z)=\text{ReLU}(\Re%20(z))+i\text{ReLU}(\Im%20(z))).

![Pub3Img1_projects](http://Lingyun-Wang.github.io/images/Pub3Img1.png)  
**Figure 1**  The network architecture of the CVSR-Net.  
In the shallow feature extraction part, two ℂConv layers with a kernel size of 3, 64 channels and a stride size of 1 learn the low-level features of the image. The features extracted by the first ℂConv layer are connected to the reconstruction part to achieve a global residual learning. This not only helps transfer the low-level features to the final image, but also avoids learning a complicated mapping of two images.  
The deep feature extraction comprises four CVSR blocks and a channel concatenation layer. As shown in Fig. 1, the basic unit in the CVSR block is built as the residual block in EDSR. The two residual blocks are utilized to extract the high-level features. The red, orange, and green curves denote dense connections as used in DenseNet. Therefore, the feature-maps in the CVSR block can be fully used and fused by the dense connections. Next, the feature channel is recompressed to 64 by a 1×1 ℂConv layer.  
The multi in Fig. 1 means that the feature-maps are multiplied by a residual scaling factor to make the training process more stable. Before the feature-map is transferred to the next CVSR block, the input feature-map is added with the local residual learning to reduce the degradation and improve the learning ability of the network. The feature-maps produced by four CVSR blocks are concatenated at the end of the deep feature extraction part to fuse the features at various levels.  
Finally, the 1×1 ℂConv layer compresses the channel number of the feature from 256 to 64, and the final image can be generated by two ℂConv layers with a kernel size of 3. Note that the output of the proposed CVSR-Net is in complex-value domain. To conduct the backpropagation and visualize the image, the amplitude of the output is utilized.  
To verify the superiority of the complex-valued networks against their corresponding real-valued counterparts, we compare the complex-valued network and its real-valued counterpart on six prevailing super-resolution networks except the proposed CVSR-Net.  
![Pub3Img3_projects](http://Lingyun-Wang.github.io/images/Pub3Img3.png)  
**Figure 2**  Comparisons between the complex-valued networks and the real-valued networks. C- and R- represents complex-valued and real-valued, respectively. The input is the image with a 25% spectral truncation.  

**Compressive sensing denoising technology for Optical Coherence Tomography Angiography (OCTA)**  
A method based on compressive sensing (CS) is proposed for reducing the acquisition time and noise of optical coherence tomography angiography (OCTA). 
It is aimed at making up for the disadvantages of OCTA including motion-induced artifact and noise. The results show that vasculature structures can be 
reconstructed well through CS on B-scans with a sampling rate of 70%. Moreover, the noise can be significantly eliminated by specially-designed 
CS-Guassian-Median (CGM) filter.  
![Projects1Image1](http://Lingyun-Wang.github.io/images/Projects1Image1.png)  
**Figure 1** (A) Schematic of the swept-source OCTA imaging system. (B) Mechanical drawing of the handheld OCT probe. BD, balanced photodetector; 
CL, collimator; FC, fiber coupler; GS, galvo scanner; L, lens; M, mirror; S, sample; SS, swept source  
![Projects1Image2](http://Lingyun-Wang.github.io/images/Projects1Image2.png)  
**Figure 2** Evaluation of CS reconstruction and CGM filter on en-face images of human ear and cheek: (A) raw image of ear. (B–C) 
the corresponding CS-reconstructed image and CGM-filtered image. (D) raw image of cheek. (E–F) the corresponding CS-reconstructed 
image and CGM-filtered image


